{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Page Protections\n",
    "This notebook shows page protections on Wikipedia via the Mediawiki API. It has two stages:\n",
    "Accessing the Page Protection API\n",
    "Analysis of page protection data (both descriptive statistics and learning a predictive model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import gzip  # necessary for decompressing dump file into text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every language on Wikipedia has its own page restrictions table\n",
    "# you can find all the dbnames (e.g., enwiki) here: https://www.mediawiki.org/w/api.php?action=sitematrix\n",
    "# for example, you could replace the LANGUAGE parameter of 'enwiki' with 'arwiki' to study Arabic Wikipedia\n",
    "LANGUAGE = 'enwiki'\n",
    "# e.g., enwiki -> en.wikipedia (this is necessary for the API section)\n",
    "SITENAME = LANGUAGE.replace('wiki', '.wikipedia')\n",
    "# directory on PAWS server that holds Wikimedia dumps\n",
    "DUMP_DIR = \"/public/dumps/public/{0}/latest/\".format(LANGUAGE)\n",
    "DUMP_FN = '{0}-latest-page_restrictions.sql.gz'.format(LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: /public/dumps/public/enwiki/latest/enwiki-latest-page_restrictions.sql.gz: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# The dataset isn't huge -- 1.1 MB -- so should be quick to process in full\n",
    "!ls -shH \"{DUMP_DIR}{DUMP_FN}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zcat: can't stat: /public/dumps/public/enwiki/latest/enwiki-latest-page_restrictions.sql.gz (/public/dumps/public/enwiki/latest/enwiki-latest-page_restrictions.sql.gz.Z): No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first 1000 characters of the page protections dump to see what it looks like\n",
    "!zcat \"{DUMP_DIR}{DUMP_FN}\" | head -46 | cut -c1-1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#import mwapi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Page Protection APIs\n",
    "NOTE:I used API to extract data from wikimedia protection page because it gives a complete and less missing data than the dump method of extracting data. Also, its gives a better output in terms of time effective characteristics of a good data analyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SITENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_protection function\n",
    "The get_protection function extract the raw protection from Wikipedia pages by making an HTTP request to mediawiki's API and parsing the response. This function accepts a page title as parameter. The aim of this function is to abstract the process of getting page protections into a reusable unit of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def get_protection(page_title):\n",
    "  S = requests.Session()\n",
    "  URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "  PARAMS = {\n",
    "      \"action\": \"query\",\n",
    "      \"format\": \"json\",\n",
    "      \"prop\": \"info\",\n",
    "      \"titles\": page_title,\n",
    "      \"inprop\": \"protection\"\n",
    "  }\n",
    "\n",
    "  R = S.get(url=URL, params=PARAMS)\n",
    "  DATA = R.json()\n",
    "\n",
    "  pages = DATA[\"query\"][\"pages\"]\n",
    "  return list(pages.values())[0]['protection']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_cities function\n",
    "get_cities downloads a JSON doc. This document containing a list of countries and their associated cities is iterated on. The get_protection is reused here to fetch the protection of each cities which are pages in Wikipedia and saved in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "CITIES = 'https://raw.githubusercontent.com/russ666/all-countries-and-cities-json/master/countries.json'\n",
    "df_cities = pd.DataFrame()\n",
    "\n",
    "def get_cities(df_cities):\n",
    "  cities = requests.get(CITIES).json()\n",
    "  for key, values in cities.items():\n",
    "    values.append(key)\n",
    "    for value in values :\n",
    "      try:\n",
    "        protections = get_protection(value)\n",
    "        for protection in protections:\n",
    "          print(protection)\n",
    "          df_cities = df_cities.append(protection, ignore_index=True)\n",
    "      except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities.to_csv(r\"cities_wiki.csv\", index=False)\n",
    "\n",
    "\n",
    "get_cities(df_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries for analysis\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import os, sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas.util.testing as tm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#importing the data extracted from APIs into the notebook to analysis, preprocess, visualise and do predict modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'cities_wiki.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check for missing data\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('type', data = data, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Basic Details\n",
    "Basic details on total number of level and total number of expiry based on protection type. Findings:\n",
    "1. For level, sysop level has a higher number on wikimedia protection page, followed by autoconfirmed and \n",
    "   the least was extendedconfirmed\n",
    "2. For expiry, infinity expiry has a higher total number on wikipediaprotection page than other finite/non-infinity expiry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('level', data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data['level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('level', data = data, hue='type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship with level and type of protection\n",
    "protection of page from being moved is higher in sysop level than edit protection of pages while, protection of page from being moved is lower in autoconfirmed and extendedconfirmed level than edit protection of pages. Whether a page is a move or edit protection seems not to matter because move protection have higher sysop level but less in autoconfirmed and extendedconfirmed on average protection page, also, move have lower autoconfirmed and extendedconfirmed level but less in sysop level on the wikimedia protection page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are two forms of encoding are available using sklearn library; one-hot and label encoding\n",
    "#For this case, we will be using the pandas method for one-hot encoding (we do not want priority) as shown in the code below.\n",
    "data = pd.get_dummies(data, columns = ['level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPIRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('expiry', data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data['expiry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to change 'expiry' from object to integer\n",
    "#categorising into infinity expiry and non-infinity expiry by infinity as 1 and the non-infinity as 0\n",
    "\n",
    "def func(data):\n",
    "    d =[]\n",
    "    for m in data:\n",
    "        if m =='infinity':\n",
    "            d.append(1)\n",
    "        else:\n",
    "            d.append(0)\n",
    "    return d\n",
    "\n",
    "data['expiry'] = func(data['expiry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relationship with expiry and type of protection\n",
    "sns.countplot('expiry', data = data, hue='type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship with expiry and type of protection\n",
    "protection of page from being moved is higher in infinity expiry than edit protection of pages while, protection of page from being moved is lower in finite/non-infinty expiry than edit protection of pages Whether a page is a move or edit protection seems to matter because move protection have higher infinity level than edit protection on average protection page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert data into a more usuable form by creating a function to change the 'type' dataset from object to integer, since there are two types of protection; edit protection and move protection\n",
    "create a function for edit(protection of page from being edited) to be 1 while move(protection of page from being moved) to be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(data):\n",
    "    d =[]\n",
    "    for m in data:\n",
    "        if m =='edit':\n",
    "            d.append(1)\n",
    "        else:\n",
    "            d.append(0)\n",
    "    return d\n",
    "\n",
    "data['type'] = func(data['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modelling\n",
    "We've established that there is a clear relationship between level, expiry and type of proctection and that relationship also depends on whether the item is a move or edit protection. Now we want to see with how much accuracy we can predict the number of level and expiry based on the protection page. This can tell us for which type of proctection we would expect wikipedia pages to have.\n",
    "NOTE: the model presented below is very simplistic and and tell us about classification of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that I have been able to process all dirt in my features i.e cleaning my data\n",
    "#I can go ahead to separate the target from the actual data using the code below.\n",
    "\n",
    "y = data['type']\n",
    "x = data.drop('type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Letâ€™s go on to data model. I will be using the library called sklearn. The algorithms I will use includes LogisticRegression and\n",
    "#RandomForestClassifier . First and foremost, I need to split the train data into \n",
    "#train and test so that I can use a percentage to train my model and the rest to evaluate the performance of the model. \n",
    "#In this case, my train will take 80% while testing will take 20% using the train_test_split function available in scikit-learn.\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,\n",
    "                                                    y, \n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = RandomForestClassifier(random_state= 42)\n",
    "rand.fit(x_train, y_train) # model learning\n",
    "\n",
    "# evaluating the train data using accuracy score \n",
    "print('Training score is:', rand.score(x_train, y_train))\n",
    "\n",
    "# make your predictions on the test data\n",
    "pred= rand.predict(x_test)\n",
    "\n",
    "# evaluate the test data using accuracy score\n",
    "print('Testing score is:', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other analysis\n",
    "Because of unbalanced dataset, I used confusion matrix for further analysis of to evaluate the performance of my model (to compute the accuracy of the algorithmn) when there is an unbalanced datasets precision, recall and f1-score shows better performance of the model than the accuaracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lr = LogisticRegression() # algorithm instantiation\n",
    "Lr.fit(x_train, y_train) # model learning\n",
    "\n",
    "# evaluating the train data using accuracy score \n",
    "print('Training score is: ', Lr.score(x_train, y_train))\n",
    "\n",
    "# make your predictions on the test data\n",
    "pred = Lr.predict(x_test)\n",
    "\n",
    "# evaluate the test data using accuracy score\n",
    "print('Testing score is: ', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the f1_score of your predictions to evaluate better performance of my model\n",
    "f1_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report of your prediction\n",
    "classification_report(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Analyses\n",
    "To predictive a better model it more data and variables should be added. For example:\n",
    "Considering using a more advanced classifier than other model types provided that there are larger dataset\n",
    "Adding other type of protection than move and edit protection E.g., create, semi protection etc.\n",
    "using other labels such as level of protection, expiry of protection to build a predictive model to know for example,\n",
    "a.whether expiry of protection page will be infinity or not.\n",
    "b.whether level of protection will be 'autoconfirmed', 'extendedconfirmed' or 'sysop' level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
